{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvsjmC27HBo9Wk/udH/zRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvanitha/DE-M7/blob/main/First_LLM_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XBHUOycE1va",
        "outputId": "29db25db-2ffc-4f67-ac9d-3c72dd2ff937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ API key found!\n",
            "âœ“ Key starts with: AIzaSyC531...\n"
          ]
        }
      ],
      "source": [
        "# Let's verify your API key is configured correctly\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"âœ“ API key found!\")\n",
        "    print(f\"âœ“ Key starts with: {api_key[:10]}...\")\n",
        "except Exception as e:\n",
        "    print(\"âœ— API key not found. Please add it using the key icon on the left.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import the Gemini library\n",
        "!pip install -q google-genai\n",
        "\n",
        "import google.genai as genai\n",
        "from google.genai import types\n",
        "from IPython.display import Markdown, display, HTML"
      ],
      "metadata": {
        "id": "UglKOmBVFCR9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the API with your key\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))"
      ],
      "metadata": {
        "id": "Yrre41y1HYNT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model instance\n",
        "# We're using 'gemini-2.5-flash' - it's fast and free!\n",
        "response_flash = client.models.generate_content(\n",
        "    model='gemini-2.5-flash',\n",
        "    contents='Explain what a data warehouse is in one sentence',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[{'code_execution': {}}]\n",
        ")\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "display(Markdown(response_flash.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "q9zqtk5tIKo5",
        "outputId": "dce85de5-62eb-48cf-9259-301ea7f7631c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A data warehouse is a centralized repository that stores integrated data from one or more disparate sources, optimized for reporting and analysis."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the response with a different model 'gemini-2.5-flash-lite'\n",
        "response_flash_lite = client.models.generate_content(\n",
        "    model='gemini-2.5-flash-lite',\n",
        "    contents='Explain what a data warehouse is in one sentence',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[{'code_execution': {}}]\n",
        ")\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "display(Markdown(response_flash_lite.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "g8G7TtfbMMpF",
        "outputId": "837835e8-8ff1-4aea-9ab4-a5e470acf6ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A data warehouse is a large, centralized repository of integrated data from various sources, designed for reporting and analysis."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "chat = client.chats.create(model='gemini-2.5-flash', history=[])"
      ],
      "metadata": {
        "id": "0ZroayAwNBEX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Send first message\n",
        "response = chat.send_message(\"I'm learning about ETL processes\")\n",
        "display(Markdown(response.text))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Send follow-up - notice it remembers we're talking about ETL\n",
        "response = chat.send_message(\"What's the difference between ETL and ELT?\")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "bGthlNA8O4Zb",
        "outputId": "97559bd9-a92d-47a4-e6e0-9203f29a0af2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's a fantastic area to learn about! ETL (Extract, Transform, Load) processes are the backbone of most data warehousing, business intelligence, and analytics initiatives.\n\nIt's a foundational process for integrating data from various sources into a unified system, typically a data warehouse or data lake, for reporting and analysis. The primary goal is to turn raw, disparate data into clean, consistent, and reliable information that can be easily queried and used to make informed business decisions.\n\nLet's break down each stage:\n\n1.  **Extract:**\n    *   **What it is:** This is the process of reading data from one or more source systems. These sources can be incredibly varied.\n    *   **Examples of Sources:**\n        *   **Relational Databases:** SQL Server, Oracle, MySQL, PostgreSQL.\n        *   **NoSQL Databases:** MongoDB, Cassandra.\n        *   **Flat Files:** CSV, XML, JSON files stored locally or in cloud storage (e.g., Amazon S3, Azure Blob Storage, Google Cloud Storage).\n        *   **APIs:** Data pulled from web services (e.g., Salesforce, Google Analytics, social media platforms).\n        *   **Streaming Data:** Real-time data from IoT devices, clickstreams, logs.\n        *   **Enterprise Applications:** SAP, Oracle EBS, CRM systems.\n    *   **Key Consideration:** The extraction process needs to be efficient and often non-intrusive to the source system's performance.\n\n2.  **Transform:**\n    *   **What it is:** This is often the most critical and complex stage. It involves applying a set of rules and functions to the extracted data to cleanse, standardize, enrich, and prepare it for the target system. This is where data quality is addressed, and data is structured for analytical purposes.\n    *   **Common Transformations:**\n        *   **Data Cleansing:** Handling missing values, correcting errors, removing duplicates, standardizing formats (e.g., date formats, postal codes).\n        *   **Data Standardization:** Ensuring consistent units of measurement, codes, or terminology across different sources.\n        *   **Data Deduplication:** Identifying and removing redundant records.\n        *   **Data Aggregation:** Summarizing data (e.g., calculating total sales per month, average product ratings).\n        *   **Data Enrichment:** Adding new information to existing data (e.g., deriving a customer's region from their zip code, looking up product categories).\n        *   **Data Filtering:** Selecting only relevant data based on certain criteria.\n        *   **Data Validation:** Checking if data conforms to business rules or constraints.\n        *   **Data Joining:** Combining data from multiple sources based on common keys.\n        *   **Data Type Conversion:** Converting data from one type to another (e.g., text to numeric).\n        *   **Applying Business Logic:** Calculating new metrics or derived attributes.\n\n3.  **Load:**\n    *   **What it is:** The final step where the transformed data is moved into the target system, which is typically a data warehouse, data mart, or data lake.\n    *   **Types of Loads:**\n        *   **Full Load:** All data from the source (or the transformed result) is loaded into the target, often overwriting previous data. This is typically done for smaller datasets or initial loads.\n        *   **Incremental Load (or Delta Load):** Only the data that has changed or been added since the last load is transferred. This is much more efficient for large datasets and frequent updates.\n    *   **Target Systems:**\n        *   **Data Warehouse:** A structured repository optimized for querying and reporting, often using dimensional modeling (star or snowflake schemas).\n        *   **Data Mart:** A subset of a data warehouse, focused on a specific business function or department.\n        *   **Data Lake:** A storage repository that holds a vast amount of raw data in its native format until it's needed.\n\n### Why is ETL Important?\n\n*   **Data Integration:** Brings disparate data sources together.\n*   **Data Quality:** Ensures data is clean, consistent, and reliable.\n*   **Performance:** Pre-processes and aggregates data, making analytical queries faster.\n*   **Historical Context:** Allows for tracking changes over time.\n*   **Business Intelligence:** Provides the foundation for reporting, dashboards, and advanced analytics.\n\n### ETL vs. ELT\n\nWith the advent of powerful, scalable cloud data warehouses (like Snowflake, Google BigQuery, Amazon Redshift), a newer paradigm called **ELT (Extract, Load, Transform)** has become very popular.\n\n*   **ETL:** Data is transformed *before* being loaded into the target. This typically involves a staging area *outside* the final data warehouse for transformations.\n*   **ELT:** Data is *first* loaded directly into the target system (often a data lake or powerful cloud data warehouse), and then transformations are performed *within* that system using its native processing capabilities (e.g., SQL on BigQuery/Snowflake).\n\n**Advantages of ELT:**\n*   Leverages the target system's immense processing power, making it faster for large datasets.\n*   More agile: You can store raw data and decide on transformations later.\n*   Simpler infrastructure: Less need for separate transformation engines.\n\n### Common ETL Tools & Technologies:\n\n*   **Traditional ETL Tools:** Informatica PowerCenter, Talend, IBM DataStage, Microsoft SSIS (SQL Server Integration Services), Oracle Data Integrator.\n*   **Cloud-Native Services:** AWS Glue, Azure Data Factory, Google Cloud Dataflow.\n*   **Orchestration Tools:** Apache Airflow, Luigi.\n*   **Data Transformation Tools (often used in ELT):** DBT (Data Build Tool).\n*   **Programming Languages/Frameworks:** Python (with libraries like Pandas, PySpark), Apache Spark.\n\n### What are you most interested in learning next?\n\n*   The different stages in more detail?\n*   Specific tools or technologies?\n*   Best practices or common challenges?\n*   A comparison of ETL vs. ELT?\n*   A practical example?\n\nLet me know how I can best help you on your learning journey!"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "You've hit on one of the most significant shifts in data architecture over the last decade! While both **ETL (Extract, Transform, Load)** and **ELT (Extract, Load, Transform)** achieve the same ultimate goal of getting data ready for analysis, they differ crucially in the *order* of the transformation and load steps, which leads to substantial differences in approach, technology, and benefits.\n\nLet's break them down:\n\n---\n\n## ETL (Extract, Transform, Load)\n\n**Traditional Approach**\n\n1.  **Extract:** Data is pulled from various source systems (databases, files, APIs).\n2.  **Transform:** The extracted data is moved to a *separate, intermediate staging area* (often a dedicated server or processing engine). Here, it's cleaned, standardized, aggregated, filtered, and reshaped according to predefined business rules and the target schema.\n3.  **Load:** The *fully transformed and clean data* is then loaded into the target data warehouse or data mart.\n\n**How it Works & Why:**\n\n*   **Pre-Cloud Era:** This approach originated when data warehouses had limited compute power for complex transformations. It was more efficient to transform data *outside* the data warehouse and then load only the finished product.\n*   **Schema-on-Write:** You typically define your target schema and transformations upfront. Data is transformed to fit that schema *before* it's written to the data warehouse.\n*   **Data Volume:** Often involves processing smaller to medium-sized datasets, or large datasets where a dedicated, powerful transformation server handles the load.\n*   **Tools:** Traditional ETL tools like Informatica PowerCenter, IBM DataStage, Microsoft SSIS, Talend, Oracle Data Integrator are designed for this paradigm.\n\n---\n\n## ELT (Extract, Load, Transform)\n\n**Modern, Cloud-Native Approach**\n\n1.  **Extract:** Data is pulled from various source systems (same as ETL).\n2.  **Load:** The *raw, untransformed data* is immediately loaded into the target system, which is typically a modern cloud data warehouse (e.g., Snowflake, Google BigQuery, Amazon Redshift) or a data lake (e.g., S3, Azure Data Lake Storage).\n3.  **Transform:** Once the raw data is in the powerful target system, transformations (cleansing, standardization, aggregation, etc.) are performed *within that system* using its native processing capabilities (usually SQL).\n\n**How it Works & Why:**\n\n*   **Cloud Computing Power:** The rise of highly scalable and performant cloud data warehouses made ELT possible and popular. These systems are designed to handle massive amounts of data and complex computations very efficiently.\n*   **Schema-on-Read:** You load the raw data first, and the schema and transformations are applied *at the time of querying or view creation*. This means you can keep raw data and define multiple transformations or \"views\" on it as needed.\n*   **Data Volume:** Ideal for very large and continuously growing datasets (Big Data) because the initial load is very fast, and transformations leverage distributed computing.\n*   **Tools:** Often involves lightweight extraction tools (e.g., Fivetran, Stitch), cloud-native services (AWS Glue, Azure Data Factory), and transformation tools that run *within* the data warehouse (e.g., DBT - Data Build Tool, or just raw SQL).\n\n---\n\n## Key Differences Summarized\n\n| Feature               | ETL (Extract, Transform, Load)                               | ELT (Extract, Load, Transform)                                      |\n| :-------------------- | :----------------------------------------------------------- | :------------------------------------------------------------------ |\n| **Order**             | **E -> T -> L**                                              | **E -> L -> T**                                                     |\n| **Transformation Location** | Separate staging server/engine, outside the target data warehouse. | *Within* the target data warehouse or data lake.                    |\n| **Data Loaded**       | Only *transformed, clean data* is loaded into the target.    | *Raw, untransformed data* is loaded first into the target.          |\n| **Data Storage**      | Raw data is often discarded or archived separately after extraction. Only processed data resides in the DW. | Raw data is preserved in the target system, alongside transformed data. |\n| **Compute Power**     | Requires dedicated compute for transformations (often on-prem). | Leverages the powerful, scalable compute of the cloud data warehouse/data lake. |\n| **Performance**       | Can be slower for initial data loading due to transformations being done externally. | Faster initial load as data is moved directly. Transformations happen in parallel within the DW. |\n| **Flexibility/Agility** | Less flexible. Changing transformations means re-running the entire ETL process, potentially losing original context. \"Schema-on-write.\" | Highly flexible. Raw data is available for new transformations, re-transformations, and varied analytical use cases. \"Schema-on-read.\" |\n| **Data Governance**   | Sensitive data can be masked/redacted *before* entering the data warehouse. | Raw, potentially sensitive data, enters the data warehouse. Requires strong internal access controls and security measures. |\n| **Cost Model**        | Upfront cost for ETL tools/infrastructure, then maintenance. | Often pay-as-you-go for cloud resources. Can be higher storage costs due to raw data, but more elastic. |\n| **Complexity**        | Managing separate ETL servers and tools can be complex.      | Simpler infrastructure. Complexity shifts to writing efficient SQL transformations. |\n| **Best For**          | Legacy systems, strict compliance where raw data must not enter, smaller datasets, on-prem setups. | Big Data, cloud-native environments, real-time analytics, agile development, data lakes. |\n\n---\n\n## When to Choose Which?\n\n**Choose ETL if:**\n\n*   You are dealing with **legacy on-premise systems** that are not designed for direct interaction with modern cloud data warehouses.\n*   You have **strict regulatory compliance** that mandates sensitive data is never stored raw, even temporarily, in the target system.\n*   Your data volumes are **smaller** and don't necessitate the scale of cloud data warehouses.\n*   You have a **well-defined data model** and transformation rules that are unlikely to change frequently.\n*   You are comfortable with **traditional ETL tools** and their infrastructure.\n\n**Choose ELT if:**\n\n*   You are building a **new data platform in the cloud** using services like Snowflake, BigQuery, Redshift, etc.\n*   You are dealing with **large volumes of data (Big Data)** where the speed of loading raw data and in-database transformation is crucial.\n*   You require **flexibility and agility** in your analytics, allowing data scientists and analysts to experiment with raw data or define new transformations without re-extracting.\n*   You want to build a **data lake** where raw, diverse data is stored for future, unforeseen analytical needs.\n*   You want to leverage the **scalability and cost-efficiency** of cloud computing.\n\nIn summary, ELT is rapidly becoming the default choice for modern data warehousing and analytics due to the power and flexibility offered by cloud technologies. However, ETL still holds its ground in specific scenarios, especially those with legacy systems or stringent compliance requirements."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a fresh chat for our interactive session\n",
        "interactive_chat = client.chats.create(model='gemini-2.5-flash', history=[])\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ¤– DATA ENGINEERING ASSISTANT\")\n",
        "print(\"=\"*80)\n",
        "print(\"Ask me anything about data engineering!\")\n",
        "print(\"Topics: SQL, ETL, Python, Data Quality, Pipelines, Warehousing...\")\n",
        "print(\"\\nType 'quit' to exit\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "        print(\"\\nðŸ‘‹ Thanks for chatting! Your agent is saved in this notebook.\")\n",
        "        break\n",
        "\n",
        "    # Send message and get response\n",
        "    try:\n",
        "        response = interactive_chat.send_message(user_input)\n",
        "        print(f\"\\nðŸ¤– Agent:\\n\")\n",
        "        display(Markdown(response.text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Try rephrasing your question.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "_VrzSvkYPwUT",
        "outputId": "044f2c62-db22-4c95-dfea-d7e7b64550bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ¤– DATA ENGINEERING ASSISTANT\n",
            "================================================================================\n",
            "Ask me anything about data engineering!\n",
            "Topics: SQL, ETL, Python, Data Quality, Pipelines, Warehousing...\n",
            "\n",
            "Type 'quit' to exit\n",
            "================================================================================\n",
            "\n",
            "You: How do I find duplicate emails?\n",
            "\n",
            "ðŸ¤– Agent:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Finding duplicate emails depends on where your email data is stored. Here are methods for the most common scenarios:\n\n---\n\n### **1. In a Spreadsheet (Excel, Google Sheets, LibreOffice Calc)**\n\nThis is the most common and versatile method if you have your email list in a column.\n\n**Method A: Using Conditional Formatting (to highlight duplicates)**\n\n1.  **Copy your emails** into a single column (e.g., Column A) in a new spreadsheet.\n2.  **Select the entire column** (e.g., Column A).\n3.  Go to:\n    *   **Excel:** `Home` tab > `Conditional Formatting` > `Highlight Cells Rules` > `Duplicate Values...`\n    *   **Google Sheets:** `Format` > `Conditional formatting` > Under `Format rules`, change `Format rules` to `Custom formula is` and enter: `=COUNTIF(A:A,A1)>1` (assuming your data is in column A). Set your formatting style (e.g., red fill).\n4.  **Choose a formatting style** (e.g., light red fill with dark red text) and click `OK`.\n5.  All duplicate email addresses will now be highlighted.\n\n**Method B: Using \"Remove Duplicates\" (to delete/filter them)**\n\n1.  **Copy your emails** into a single column.\n2.  **Select the entire column** (or the range containing your data).\n3.  Go to:\n    *   **Excel:** `Data` tab > `Data Tools` group > `Remove Duplicates`. Ensure only the column containing emails is selected, then click `OK`. Excel will tell you how many duplicates were found and removed.\n    *   **Google Sheets:** `Data` > `Data cleanup` > `Remove duplicates`. Select the relevant column(s) and click `Remove duplicates`.\n4.  **Important:** This method permanently removes the duplicate rows. If you only want to *find* them, use Conditional Formatting first, or copy your data to a new sheet before using \"Remove Duplicates.\"\n\n**Method C: Using a Formula (to identify and count them)**\n\n1.  **Copy your emails** into a single column (e.g., Column A).\n2.  In an adjacent column (e.g., Column B), enter the following formula in the first cell (e.g., B1):\n    *   `=COUNTIF(A:A, A1)`\n3.  Drag the fill handle (the small square at the bottom right of B1) down to apply the formula to all cells in Column B.\n4.  Any email address with a number greater than 1 in Column B is a duplicate. You can then filter Column B for values > 1 to see all duplicates.\n\n---\n\n### **2. In Email Clients (Outlook, Gmail, Thunderbird, Apple Mail)**\n\nThese clients usually focus on contact management rather than finding duplicate emails *within* your inbox messages.\n\n**Outlook (Contacts):**\n\n1.  Go to the **People** (or Contacts) section.\n2.  Look for tools like **\"Clean Up Contacts\"** or **\"Link Duplicates\"**.\n    *   In newer Outlook versions, Outlook often automatically links duplicate contacts or prompts you to merge them.\n    *   If you have a large list, you might need to export contacts to a CSV and use a spreadsheet method.\n\n**Gmail (Google Contacts):**\n\n1.  Go to [Google Contacts](https://contacts.google.com/).\n2.  In the left sidebar, click **\"Merge & fix\"** (or sometimes it's labeled \"Find duplicates\").\n3.  Google will suggest duplicate contacts that it can merge. Review and confirm.\n4.  If you want to perform a more robust check, you can export your Google Contacts (`Export` option in the left sidebar) to a CSV and then use a spreadsheet method.\n\n**Thunderbird / Apple Mail:**\n\n*   These clients typically don't have built-in \"find duplicates\" features for contacts. Your best bet is to **export your contacts** to a CSV file and then use one of the spreadsheet methods above.\n\n---\n\n### **3. In CRMs or Marketing Automation Platforms (e.g., Salesforce, HubSpot, Mailchimp)**\n\nMost professional platforms have built-in de-duplication features, though they vary by system:\n\n*   **During Import:** Many platforms will automatically detect and either merge or flag duplicate records during the import process.\n*   **Built-in Tools:** Check the \"Contacts,\" \"Leads,\" or \"Audience\" section for features like:\n    *   \"Merge Records\"\n    *   \"De-duplication Tools\"\n    *   \"Clean Up Audience\"\n*   **Documentation:** Refer to the specific platform's help documentation for the most accurate instructions.\n*   **Export and Analyze:** As a last resort, you can always export your contact list and use the spreadsheet methods.\n\n---\n\n### **4. Using a Database (SQL)**\n\nIf your emails are stored in a database, you can use SQL queries.\n\n**To find duplicate emails and their counts:**\n\n```sql\nSELECT email_address, COUNT(*)\nFROM your_table_name\nGROUP BY email_address\nHAVING COUNT(*) > 1;\n```\n\n**To retrieve all rows for duplicate emails:**\n\n```sql\nSELECT t1.*\nFROM your_table_name t1\nJOIN (\n    SELECT email_address\n    FROM your_table_name\n    GROUP BY email_address\n    HAVING COUNT(*) > 1\n) AS duplicates ON t1.email_address = duplicates.email_address\nORDER BY t1.email_address;\n```\n\n---\n\n### **5. Using a Script (e.g., Python)**\n\nFor large, complex files or custom logic, a script is powerful.\n\n```python\nfrom collections import Counter\n\ndef find_duplicate_emails(file_path):\n    with open(file_path, 'r') as f:\n        emails = [line.strip().lower() for line in f if line.strip()] # Read, trim, lowercase\n\n    email_counts = Counter(emails)\n    duplicates = {email: count for email, count in email_counts.items() if count > 1}\n\n    if duplicates:\n        print(\"Duplicate emails found:\")\n        for email, count in duplicates.items():\n            print(f\"- {email} (appears {count} times)\")\n    else:\n        print(\"No duplicate emails found.\")\n\nif __name__ == \"__main__\":\n    # Create a dummy file for testing\n    with open(\"emails.txt\", \"w\") as f:\n        f.write(\"test@example.com\\n\")\n        f.write(\"user@domain.com\\n\")\n        f.write(\"Test@example.com\\n\") # Duplicate (case-insensitive)\n        f.write(\"another@email.com\\n\")\n        f.write(\"user@domain.com\\n\") # Duplicate\n\n    find_duplicate_emails(\"emails.txt\")\n```\n\n---\n\n### **Important Considerations:**\n\n*   **Case Sensitivity:** \"email@example.com\" and \"Email@example.com\" might be treated as different by some tools. It's often best practice to convert all emails to lowercase before checking for duplicates (as in the Python example).\n*   **Whitespace:** Leading or trailing spaces (\" email@example.com\") can prevent detection. Trim whitespace before checking.\n*   **Sub-addressing/Aliasing:** \"user+tag@gmail.com\" and \"user@gmail.com\" often refer to the same inbox. Decide if you want to consider these duplicates. Most standard tools won't automatically do this.\n*   **What to do with them?** Once found, decide if you want to delete them, mark them, or merge associated contact information.\n*   **Backup:** Always back up your data before performing any mass deletion or modification.\n\nChoose the method that best fits where your email data currently resides and your technical comfort level. For most users, spreadsheet methods are the easiest and most accessible."
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3948685631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Exit condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}